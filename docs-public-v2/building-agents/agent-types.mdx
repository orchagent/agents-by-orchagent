---
title: "Agents & Skills"
description: "Understanding the two object types on orchagent"
---

orchagent hosts two object types: **agents** that execute tasks, and **skills** that provide reusable knowledge.

<Tip>
**Building with an AI coding assistant?** Install the agent-builder skill to give your AI the complete platform reference — sandbox contracts, boilerplate code, environment details, and debugging patterns:

```bash
orch skill install orchagent/agent-builder
```

This works with Claude Code, Cursor, Amp, and other AI tools. Your AI will have everything it needs to build agents on orchagent without trial and error.
</Tip>

## Agents vs Skills: The Simple Rule

| | Agents | Skills |
|---|--------|--------|
| **What they do** | **DO** things (execute tasks) | **TEACH** things (provide knowledge) |
| **Nature** | Active executors | Passive knowledge |
| **Commands** | `orch run`, `orch install` | `orch skill install` |
| **Output** | Structured results | Context files for AI tools |

<Info>
**Quick distinction:** Agents *do* things. Skills *inform* agents how to do things better.
</Info>

## The Agent Model

Every agent on orchagent is `type: "agent"`. What varies is **how** the agent executes — and the platform figures that out automatically from your manifest declarations.

| You declare... | Platform infers | What happens |
|----------------|-----------------|-------------|
| Nothing (just prompt + schema) | `direct_llm` | Single LLM call with your prompt |
| `loop` config | `managed_loop` | LLM tool-use loop in a sandbox |
| `runtime.command` | `code_runtime` | Your code runs in a sandbox |

You also control **when** the agent runs:

| `run_mode` | Behavior |
|------------|----------|
| `on_demand` (default) | Each call is independent — run via CLI, API, or schedule |
| `always_on` | Persistent service — Discord bots, webhook listeners, workers |

And whether the agent is **callable** by other agents:

| `callable` | Behavior |
|------------|----------|
| `false` (default) | Only users can call this agent |
| `true` | Other agents can call this agent as a dependency |

<Tip>
You don't choose an execution engine — the platform infers it from what you declare in your manifest. Just describe what your agent needs, and orchagent handles the rest.
</Tip>

## Which Pattern Should I Use?

```
I want to build something on orchagent
  |
  |-- Does it need an LLM to think and decide?
  |   |
  |   |-- Yes
  |   |   |
  |   |   |-- Simple (one call, no tools) --> prompt + schema (direct_llm)
  |   |   |   "Just an LLM call with your prompt and schema"
  |   |   |
  |   |   \-- Complex (tools, iteration, sandbox) --> loop config (managed_loop)
  |   |       "Give it a prompt + tools, platform runs the loop"
  |   |
  |   \-- No --> runtime.command (code_runtime)
  |       "Your code, our sandbox. Deterministic, fast, cheap."
  |
  \-- I want to share knowledge --> Skill
      "Markdown instructions that enhance any agent"
```

<Tip>
**Start with the simplest pattern that works.** Most use cases need only a prompt and schema. If you need tools, add a `loop` config. Only reach for `runtime.command` when you need full programmatic control or don't need an LLM.
</Tip>

---

## Direct LLM Agents (Prompt + Schema)

The simplest pattern. You provide a prompt template with variable placeholders, and orchagent handles the LLM call.

**When to use:**
- Single LLM call is sufficient
- No external API calls needed
- No complex logic or branching

**What you provide:**
```
my-agent/
+-- orchagent.json      # Manifest (type: "agent")
+-- prompt.md           # Your prompt template
+-- schema.json         # Input/output schemas (optional)
\-- README.md           # Documentation (optional)
```

### Example

**`orchagent.json`:**
```json
{
  "name": "sentiment-analyzer",
  "type": "agent",
  "description": "Analyze sentiment of text",
  "supported_providers": ["openai", "anthropic"]
}
```

**`prompt.md`:**
```markdown
Analyze the sentiment of the following text and return a JSON object
with 'sentiment' (positive, negative, or neutral) and 'confidence' (0-1).

Text: {{text}}
```

**`schema.json`:**
```json
{
  "input": {
    "type": "object",
    "properties": {
      "text": { "type": "string", "description": "Text to analyze" }
    },
    "required": ["text"]
  },
  "output": {
    "type": "object",
    "properties": {
      "sentiment": { "type": "string", "enum": ["positive", "negative", "neutral"] },
      "confidence": { "type": "number", "minimum": 0, "maximum": 1 }
    }
  }
}
```

### Prompt Variables

Use `{{variable}}` syntax in your `prompt.md`:

```
Summarize the following {{document_type}} in {{language}}:

{{content}}

Focus on: {{focus_areas}}
```

Variables are replaced with input values at runtime. All template variables must be provided with non-empty values — the API returns a `400 MISSING_INPUT_FIELDS` error listing any that are missing.

---

## Managed Loop Agents (LLM + Tools)

Managed loop agents give the LLM a **tool-use loop inside a sandbox**. Think of it as "Claude Code in a container, configured by you." The platform provides built-in tools (bash, file read/write, list files) and you can define custom command-wrapper tools. The LLM iterates autonomously until it solves the task and submits a result.

**When to use:**
- The task requires running commands, reading/writing files, or iterating
- You want the LLM to figure out the steps, not hard-code them
- You'd otherwise write code just to orchestrate LLM + subprocess calls

**What you provide:**
```
my-agent/
+-- orchagent.json      # Manifest (type: "agent", loop + custom_tools)
+-- prompt.md           # Agent instructions (system prompt)
+-- schema.json         # Input/output schemas (optional)
+-- Dockerfile          # Custom environment (optional)
\-- requirements.txt    # Extra sandbox deps (optional)
```

**What you declare in the manifest:**

```json
{
  "name": "cairo-test-engineer",
  "type": "agent",
  "description": "Fixes Cairo code until tests pass",
  "supported_providers": ["anthropic"],
  "loop": {
    "max_turns": 30
  },
  "timeout_seconds": 300,
  "custom_tools": [
    {
      "name": "run_tests",
      "description": "Run the Cairo test suite with snforge",
      "command": "snforge test"
    },
    {
      "name": "build_project",
      "description": "Build the scarb project",
      "command": "scarb build"
    }
  ]
}
```

**What the platform provides:**
1. E2B sandbox with your custom environment (if Dockerfile provided)
2. Built-in tools: `bash`, `read_file`, `write_file`, `list_files`, `submit_result`
3. Your custom tools converted to named tool definitions
4. A managed loop that runs until the LLM calls `submit_result` or hits `max_turns`

### Custom Tools

Custom tools are command wrappers that give the LLM clean, named operations instead of having to guess shell commands.

**Simple tools** (no parameters):
```json
{
  "name": "run_tests",
  "description": "Run the test suite",
  "command": "pytest"
}
```

**Tools with parameters** (use `{{param}}` placeholders):
```json
{
  "name": "deploy",
  "description": "Deploy to the specified network",
  "command": "sncast deploy --network {{network}}",
  "input_schema": {
    "type": "object",
    "properties": {
      "network": { "type": "string", "description": "Target network (testnet/mainnet)" }
    },
    "required": ["network"]
  }
}
```

The LLM sees named tools like `run_tests` and `deploy` instead of guessing raw bash commands. Bash is always available as a fallback for ad-hoc commands.

### Built-in Tools

Every managed loop agent automatically gets these tools:

| Tool | Description |
|------|-------------|
| `bash` | Run shell commands (120s per-command timeout) |
| `read_file` | Read file contents |
| `write_file` | Write/create files (auto-creates parent directories) |
| `list_files` | List directory contents (optional recursive) |
| `submit_result` | Submit final structured output and end the loop |

### Safety Limits

| Limit | Default | Configurable |
|-------|---------|-------------|
| `loop.max_turns` | 25 | Yes, in orchagent.json (platform max: 50) |
| Per-command timeout | 120 seconds | No |
| Overall timeout | Agent's `timeout_seconds` | Yes |

### Provider Support

Managed loop agents currently support **Anthropic only** (Claude). This is because Anthropic has the best tool-use support. More providers will be added in the future.

---

## Code Runtime Agents (Your Code)

Code runtime agents run your Python or JavaScript in **E2B sandboxes** — secure, isolated environments. Each call spins up a fresh sandbox, runs your script, and returns the result. You have full control over everything.

**When to use:**
- You need full programmatic control over the execution flow
- Your use case doesn't need an LLM at all (pure data processing, file conversion, etc.)
- You need multi-model orchestration (calling different LLMs for different steps)
- You have an existing codebase you want to wrap as an agent
- Managed loop agents don't give you enough control

**What you declare in the manifest:**

<Tabs>
  <Tab title="Python">
    ```json
    {
      "name": "leak-finder",
      "type": "agent",
      "description": "Finds leaked secrets in codebases",
      "supported_providers": ["gemini"],
      "runtime": {
        "command": "python main.py"
      }
    }
    ```
  </Tab>
  <Tab title="JavaScript">
    ```json
    {
      "name": "leak-finder",
      "type": "agent",
      "description": "Finds leaked secrets in codebases",
      "supported_providers": ["gemini"],
      "runtime": {
        "command": "node main.js"
      }
    }
    ```
  </Tab>
</Tabs>

### Example

<Tabs>
  <Tab title="Python">
    ```python
    # main.py
    import json
    import sys

    def main():
        # Read input from stdin
        input_data = json.load(sys.stdin)
        repo_url = input_data.get("repo_url")

        # Your logic here: clone repo, scan files, call LLM, etc.
        result = {
            "issues": ["Found hardcoded API key in config.py"],
            "risk_score": 0.7
        }

        # Write output to stdout
        print(json.dumps(result))

    if __name__ == "__main__":
        main()
    ```
  </Tab>
  <Tab title="JavaScript">
    ```javascript
    // main.js
    const fs = require('fs');

    function main() {
      const inputData = JSON.parse(fs.readFileSync('/dev/stdin', 'utf-8'));
      const repoUrl = inputData.repo_url;

      // Your logic here: clone repo, scan files, call LLM, etc.
      const result = {
        issues: ['Found hardcoded API key in config.py'],
        risk_score: 0.7,
      };

      // Write output to stdout
      console.log(JSON.stringify(result));
    }

    main();
    ```
  </Tab>
</Tabs>

### Input/Output Contract

Code runtime agents communicate via stdin/stdout as JSON.

**Standard input:**
```json
{"repo_url": "https://github.com/user/repo"}
```

**File uploads:** When files are uploaded, you receive a manifest:
```json
{
  "files": [
    {
      "path": "/tmp/uploads/invoice.pdf",
      "original_name": "invoice.pdf",
      "content_type": "application/pdf",
      "size_bytes": 1234567
    }
  ]
}
```

**Standard output:**
```json
{"issues": ["..."], "risk_score": 0.7}
```

### Directory Structure

<Tabs>
  <Tab title="Python">
    ```
    my-agent/
    +-- orchagent.json      # Agent manifest
    +-- main.py             # Entry point
    +-- requirements.txt    # Dependencies
    \-- README.md           # Documentation (optional)
    ```
  </Tab>
  <Tab title="JavaScript">
    ```
    my-agent/
    +-- orchagent.json      # Agent manifest
    +-- main.js             # Entry point
    +-- package.json        # Dependencies
    \-- README.md           # Documentation (optional)
    ```
  </Tab>
</Tabs>

The CLI auto-detects entrypoints: `main.py`, `app.py`, `index.py`, `main.js`, `index.js`. Override with:

```json
{"entrypoint": "run.py"}
```

### Skills in Code Runtime Agents

Code runtime agents can access skills at runtime. When skills are passed via the `--skills` flag or `X-Orchagent-Skills` header, they are mounted as files in your sandbox:

```python
import os
from pathlib import Path

skills_dir = os.environ.get("ORCHAGENT_SKILLS_DIR")
if skills_dir:
    skills_path = Path(skills_dir)

    # Read all skill files
    for skill_file in skills_path.glob("*.md"):
        content = skill_file.read_text()
        # Use skill content in your prompts or logic

    # Or read the manifest for metadata
    import json
    manifest = json.loads((skills_path / "manifest.json").read_text())
    for skill in manifest:
        print(f"Skill: {skill['org']}/{skill['name']}@{skill['version']}")
```

Skills are written to `/home/user/orchagent/skills/` with filenames like `org_name_version.md`. A `manifest.json` file provides metadata for programmatic access.

---

## Skills

Skills are passive knowledge — markdown files containing instructions, rules, or expertise that enhance agents.

**Use cases:**
- Coding standards (React patterns, security rules)
- Domain knowledge (legal requirements, company policies)
- Writing guidelines (tone, formatting, brand voice)

### SKILL.md Format

Skills use the [Agent Skills](https://agentskills.io) standard:

```markdown
---
name: react-best-practices
description: React optimization patterns for performance-critical apps
license: MIT
metadata:
  author: yourname
  version: "1.0"
---

## Rules

- Use functional components over class components
- Memoize expensive computations with useMemo
- Avoid inline function definitions in JSX
```

### Frontmatter Fields

| Field | Required | Description |
|-------|----------|-------------|
| `name` | Yes | Lowercase, hyphens only, max 64 chars |
| `description` | Yes | When to use this skill (max 1024 chars) |
| `license` | No | e.g., MIT |
| `metadata` | No | Author, version, etc. |

### Using Skills

**Install locally** for any AI coding tool:

```bash
# Install to current project
orch skill install yourorg/react-best-practices

# Install globally (available in all projects)
orch skill install yourorg/react-best-practices --global

# Install to specific formats only
orch skill install yourorg/react-best-practices --format claude-code,cursor
```

Writes to `.claude/skills/`, `.cursor/skills/`, `.codex/skills/`, `.agent/skills/`.

**Compose with agents** at run time:

```bash
orch run yourorg/code-reviewer --skills yourorg/react-best-practices
```

### Using Agents as Sub-Agents

Export agents as sub-agent configuration files for AI tools:

```bash
# Install agent as Claude Code sub-agent
orch install yourorg/code-reviewer

# Install to Cursor
orch install yourorg/code-reviewer --format cursor

# Install to project only
orch install yourorg/code-reviewer --scope project

# Update installed agents
orch update
```

See [CLI Commands](/using-agents/cli-commands#install) for full details.

---

## LLM Provider Configuration

Specify supported providers in your manifest:

```json
{"supported_providers": ["openai", "anthropic", "gemini"]}
```

Use `"any"` if your agent works with any provider:

```json
{"supported_providers": ["any"]}
```

<Note>
Managed loop agents (those with a `loop` config) currently only support `"anthropic"`. This will be expanded in the future.
</Note>

---

## Choosing the Right Pattern

| Create a **Skill** when... | Create a **direct LLM** agent when... | Create a **managed loop** agent when... | Create a **code runtime** agent when... |
|---------------------------|----------------------------------|------------------------------------|---------------------------------|
| You have knowledge to share | A single LLM call is enough | The LLM needs to use tools and iterate | You need full programmatic control |
| Content is reusable across contexts | No sandbox or file access needed | You'd otherwise write boilerplate orchestration | Your use case doesn't need an LLM |
| No structured I/O needed | Input/output is straightforward | The task involves running commands or editing files | You need multi-model orchestration |
| Instructions could help any agent | No retry logic needed | You want the LLM to figure out the steps | You have existing code to wrap |

---

## Migration Note

<Info>
**February 11, 2026:** The agent model has been simplified. Legacy type values (`prompt`, `tool`, `code`, `agentic`) are still accepted by the API and CLI for backward compatibility, but canonical values (`agent`, `skill`) are required for all new documentation and recommended for new projects.

Legacy types map as follows:
- `prompt` → `agent` (execution engine: `direct_llm`)
- `tool` / `code` → `agent` (execution engine: `code_runtime`)
- `agentic` → `agent` (execution engine: `managed_loop`)
- `skill` → `skill`

The `execution_engine` field (`direct_llm`, `managed_loop`, `code_runtime`) is an internal implementation detail inferred at publish time from your manifest declarations. You do not need to set it manually.
</Info>

## Next Steps

<CardGroup cols={2}>
  <Card title="Manifest Format" icon="file-code" href="/building-agents/manifest-format">
    Full orchagent.json schema
  </Card>
  <Card title="Publishing" icon="upload" href="/building-agents/publishing">
    Publish your agent or skill
  </Card>
  <Card title="Orchestration" icon="diagram-project" href="/building-agents/orchestration">
    Compose agents and skills
  </Card>
  <Card title="Agent Builder Skill" icon="wand-magic-sparkles">
    Run `orch skill install orchagent/agent-builder` to give your AI coding tool the complete platform reference for building agents.
  </Card>
</CardGroup>
